{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta prática você irá implementar o indexador para, logo após, indexar o conteúdo da Wikipédia. Nesta prática, o índice é composto pela classe abstrata `Index` que armazena a estrutura do índice e possui as operações básicas do mesmo. \n",
    "\n",
    "Iremos fazer duas implementações desse índice: o `HashIndex` que será um índice simples em memória principal e o `FileIndex` em que as ocorrências ficarão em memória secundária para possibilitar a indexação de uma quantidade maior de páginas. Assim, teremos os seguintes arquivos:\n",
    "\n",
    "- `structure.py`: Possui toda a estrutura do índice;\n",
    "- `index_structure_test.py`: Testa a estrutura do índice;\n",
    "- `file_index_test.py`: Possui os testes unitários específicos para a indexação das ocorrências em arquivos da classe `FileIndex`;\n",
    "- `performance_test.py`: Executa um teste de performance (tempo de execução e memória utilizada) do índice;\n",
    "- `indexer.py`: Possui as classes para o preprocessamento e preparação para a indexação;\n",
    "- `indexer_test.py`: Realiza o [teste de integração](https://en.wikipedia.org/wiki/Integration_testing) da prática como um todo (inclusive as funções da indexer.py).\n",
    "\n",
    "Na entrega, não esqueça de apresentar a saída de execução de cada atividade desta tarefa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação da classe Abstrata `Index` e classe `HashIndex`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe abstrata Index, no arquivo `structure.py` possui os seguintes atributos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dic_index`:  dicionário em que a chave é o termo indexação (string, gerenciado por esta classe) e, os valores,  podem dar de diferentes tipos - dependendo da subclasse;\n",
    "- `set_documents`: conjunto de ids de documentos existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os métodos serão discutidos ao longo das atividades. Inicialmente, iremos fazer a importação do módulo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index.structure import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 1 - método index da classe Index**: Este método está quase todo pronto e é responsável por indexar um termo com sua frequência e documento no índice, de acordo com uma de suas subclasses. Você deve deve apenas inicializar a variável `int_term_id` apropriadamente - substituindo os `None` correspondente. Caso o termo não exista no índice, deverá obter o próximo term_id. Esse id pode ser sequencial. Caso esse id seja encontrado, a classe Index deverá chamar o método `get_term_id` (implementado pelas subclasses) para obtê-lo pois, dependendo da implementação, haverá uma forma diferente de obtenção. Além disso, você deverá atualizar o atributo `set_documents` apropriadamente. Para testar tanto esta atividade e a seguinte,  você deverá fazer uma das implementações dessa classe - a classe **HashIndex** - na atividade 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 2 - atributos calculados da classe Index**: Você deve implementar os atributos calculados `document_count` e `vocabulary` da classe Index. O atributo `document_count` retorna a quantidade de documentos existentes e, `vocabulary` retorna uma lista com o vocabulário completo indexado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 3 - Implementação da classe HashIndex: ** O HashIndex deverá fazer um índice em memória.\n",
    "\n",
    "Como exemplo, caso tenhamos três documentos $d_1 = $\"A casa verde é uma casa bonita\", $d_2 = $\"A casa bonita\" e $d_3 =$\"O prédio verde\", caso não haja remoção de _stopwords_ nem acentos, o atributo `dic_index` deverá possuir a seguinte estrutura:"
   ]
  },
  {
   "source": [
    "{\"a\": [TermOccurrence(1,1,1), TermOccurrence(2,1,1)],\n",
    " \"casa\": [TermOccurrence(1,2,2), TermOccurrence(2,2,1)],\n",
    " \"verde\": [TermOccurrence(1,3,1), TermOccurrence(3,3,1)],\n",
    " \"é\": [TermOccurrence(1,4,1)],\n",
    " \"uma\": [TermOccurrence(1,5,1)],\n",
    " \"bonita\": [TermOccurrence(1,6,1),TermOccurrence(2,6,1)],\n",
    " \"o\": [TermOccurrence(3,7,1)],\n",
    " \"prédio\": [TermOccurrence(3,8,1)]\n",
    "}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por simplicidade deste indice, perceba que deixamos o `term_id` de forma repetida. Iremos deixar assim, porém poderiamos retirar essa redundancia para reduzir o consumo de memória. Nesta prática, iremos implementar o índice em arquivo que irá ser melhor ainda na questão de consumo de memória ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O índice é chamado da seguinte forma - esse código só ira funcionar depois que vocẽ terminar esta atividade :):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = HashIndex()\n",
    "#indexação do documento 1\n",
    "index.index(\"a\",1,1)\n",
    "index.index(\"casa\",1,2)\n",
    "index.index(\"verde\",1,1)\n",
    "index.index(\"é\",1,1)\n",
    "index.index(\"uma\",1,1)\n",
    "#indexação do documento 2\n",
    "index.index(\"a\",2,1)\n",
    "index.index(\"casa\",2,1)\n",
    "index.index(\"bonita\",2,1)\n",
    "\n",
    "#indexação do documento 3\n",
    "index.index(\"o\",3,1)\n",
    "index.index(\"prédio\",3,1)\n",
    "index.index(\"verde\",3,1)\n",
    "\n",
    "index.finish_indexing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe `Index` é que irá manter o dicinário com o vocabulário do índice e, dependendo de sua implementação, suas ocorrências. Iremos fazer duas implementações: a classe `HashIndex`, que faz o indice e suas ocorrências em memória principal e a classe `FileIndex`, que armazena as ocorrências em arquivo, ambas subclasses de `Index`. O método `finish_indexing` é um método que não está implementado no `Index` e é implementado (opcionalmente) nas suas subclasses caso haja necessidade de fazer algo no final da indexação. Em nosso caso, apenas a classe `FileIndex` irá precisar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, você deverá implementar a classe HashIndex. Para sua implementação, você deverá completar os seguintes métodos/atributo calculados:\n",
    "- `create_index_entry`: cria uma nova entrada no índice utilizando, se necessário, o id do termo passado como parâmetro - não será necessário agora. A implementação deste método é **super simples** - apenas substitua o None. Verifique também o método `index` da superclasse para entender melhor o que será retornado;\n",
    "- `add_index_occur`: Adiciona uma nova ocorrência na entrada deste índice. Você precisará da entrada do termo atual, o id do documento e frequência do termo no documento, passado como parâmetro. Atualize substituindo os `None`. Veja também o modo `index` da superclasse para entender melhor como o método funciona.\n",
    "\n",
    "Faça os testes baixo para garantir que a atividade atual e as duas anteriores foram implementadas corretamente. Nos primeiros dois testes, você ainda não verá a lista de ocorrências, pois o método para obtê-la será implementado a seguir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======= Indice Gerado ======\ncasa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\nvermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\nverde -> [(term_id:2 doc: 1 freq: 1)]\n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "!python -m index.index_structure_test StructureTest.test_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======= Indice Gerado ======\ncasa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\nvermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\nverde -> [(term_id:2 doc: 1 freq: 1)]\n.\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "!python -m index.index_structure_test StructureTest.test_document_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, implemente os métodos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `get_occurrence_list`: Retornará a lista de ocorrências de um determinado termo. Considerando o exemplo apresentado no início desta atividade, `index.get_occurrence_list('casa')` retornará a lista `[TermOccurrence(1,2,2), TermOccurrence(2,2,1)]`. Caso um termo não exista, este método deverá retornar uma lista vazia. Logo após, execute o teste abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======= Indice Gerado ======\ncasa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\nvermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\nverde -> [(term_id:2 doc: 1 freq: 1)]\n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "!python -m index.index_structure_test StructureTest.test_get_occurrence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `document_count_with_term`: Retorna a quantidade de documentos que possuem um determinado termo. Considerando o exemplo apresentado no início desta atividade, `index.document_count_with_term('casa')` retornará 2. Caso um termo não exista, este método deverá retornar zero. Logo após, execute o teste abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======= Indice Gerado ======\ncasa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\nvermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\nverde -> [(term_id:2 doc: 1 freq: 1)]\n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "!python -m index.index_structure_test StructureTest.test_document_count_with_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 4 - métodos de comparação da classe TermOccurrence**: Eventualmente iremos precisar ordenar as ocorrências Por isso, temos que implementar os [comparadores de `__eq__` e `__lt__`](https://docs.python.org/3.7/reference/datamodel.html#object.__lt__) além de usar o _decorator_ [total_ordering](https://docs.python.org/3.7/library/functools.html#functools.total_ordering) - [veja também aqui](https://portingguide.readthedocs.io/en/latest/comparisons.html#rich-comparisons). `__eq__` retorna igual se um objeto é considerado igual ao outro. Considere que uma ocorrência é igual a outra se o id do termo dela e o id do documento forem iguais. \n",
    "\n",
    "O comparador `<` é implementado pelo método `__lt__` que retorna verdadeiro se o objeto corrente `self` é menor do que o objeto passado como parâmetro. A ocorrência deverá ser ordenada primeiramente pelo seu `term_id` e, logo após, pelo `doc_id`. Faça o exemplo abaixo para testar (não esqueça de reiniciar o Kernel quando modificar o código):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Resultado obtido: False - esperado: False\n2 Resultado obtido: False - esperado: False\n3 Resultado obtido: False - esperado: False\n4 Resultado obtido: True - esperado: True\n5 Resultado obtido: False - esperado: False\n6 Resultado obtido: True - esperado: True\n7 Resultado obtido: False - esperado: False\n8 Resultado obtido: False - esperado: False\n"
     ]
    }
   ],
   "source": [
    "from index.structure import *\n",
    "t1 = TermOccurrence(1,1,2)\n",
    "t2 = TermOccurrence(3,1,2)\n",
    "t3 = TermOccurrence(1,2,2)\n",
    "t4 = TermOccurrence(2,2,2)\n",
    "t5 = TermOccurrence(2,2,2)\n",
    "\n",
    "\n",
    "print(f\"1 Resultado obtido: {t1 == t5} - esperado: False\")\n",
    "print(f\"2 Resultado obtido: {t1 != t1} - esperado: False\")\n",
    "print(f\"3 Resultado obtido: {t1 == None} - esperado: False\")\n",
    "print(f\"4 Resultado obtido: {t1 < t2} - esperado: True\")\n",
    "print(f\"5 Resultado obtido: {t2 > t3} - esperado: False\")\n",
    "print(f\"6 Resultado obtido: {t3 < t4} - esperado: True\")\n",
    "print(f\"7 Resultado obtido: {t2 > t4} - esperado: False\")\n",
    "print(f\"8 Resultado obtido: {t2 > None} - esperado: False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção de índice usando arquivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A construção de índice usando apenas memória principal fácil de implementar e eficiente em termos de tempo de execução. Porém, quando precisamos de indexar milhões/bilhões de páginas, é muitas vezes inviável armazenarmos tudo em memória principal. \n",
    "\n",
    "Para resolver esse problema, uma solução é mantermos o vocabulário em memória principal e as ocorrências em memória secundária. Assim, teríamos o mesmo atributo `dic_index` na classe `Index`. Porém, cada entrada (termo) referenciará as ocorrências em arquivo. Utilizando exemplo da atividade 3 neste contexto, no final da indexação, o `dic_index` deve ficar da seguinte forma: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'a': term_id: 1, doc_count_with_term: 2, term_file_start_pos: 0,\n",
       " 'casa': term_id: 2, doc_count_with_term: 2, term_file_start_pos: 20,\n",
       " 'verde': term_id: 3, doc_count_with_term: 2, term_file_start_pos: 40,\n",
       " 'é': term_id: 4, doc_count_with_term: 1, term_file_start_pos: 60,\n",
       " 'uma': term_id: 5, doc_count_with_term: 1, term_file_start_pos: 70,\n",
       " 'bonita': term_id: 6, doc_count_with_term: 2, term_file_start_pos: 80,\n",
       " 'o': term_id: 7, doc_count_with_term: 1, term_file_start_pos: 100,\n",
       " 'prédio': term_id: 8, doc_count_with_term: 1, term_file_start_pos: 110}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "{\"a\": TermFilePosition(1, 0, 2), \n",
    " \"casa\": TermFilePosition(2, 20, 2), \n",
    " \"verde\": TermFilePosition(3, 40, 2),\n",
    " \"é\": TermFilePosition(4, 60, 1), \n",
    " \"uma\": TermFilePosition(5, 70, 1), \n",
    " \"bonita\": TermFilePosition(6, 80, 2), \n",
    " \"o\": TermFilePosition(7, 100, 1), \n",
    " \"prédio\": TermFilePosition(8, 110, 1), \n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e as ocorrências, ordenadas por termo e, logo após, por documento ficariam em um arquivo na seguinte ordem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(term_id:1 doc: 1 freq: 1),\n",
       " (term_id:1 doc: 2 freq: 1),\n",
       " (term_id:2 doc: 1 freq: 2),\n",
       " (term_id:2 doc: 2 freq: 1),\n",
       " (term_id:3 doc: 1 freq: 1),\n",
       " (term_id:3 doc: 3 freq: 1),\n",
       " (term_id:4 doc: 1 freq: 1),\n",
       " (term_id:5 doc: 1 freq: 1),\n",
       " (term_id:6 doc: 1 freq: 1),\n",
       " (term_id:6 doc: 2 freq: 1),\n",
       " (term_id:7 doc: 3 freq: 1),\n",
       " (term_id:8 doc: 3 freq: 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "[TermOccurrence(1,1,1), \n",
    " TermOccurrence(2,1,1), \n",
    " TermOccurrence(1,2,2), \n",
    " TermOccurrence(2,2,1),\n",
    " TermOccurrence(1,3,1), \n",
    " TermOccurrence(3,3,1),\n",
    " TermOccurrence(1,4,1),\n",
    " TermOccurrence(1,5,1),\n",
    " TermOccurrence(1,6,1),\n",
    " TermOccurrence(2,6,1),\n",
    " TermOccurrence(3,7,1),\n",
    " TermOccurrence(3,8,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "em que cada instância da classe `TermFilePosition` é a especificação da posição inicial de um `term_id` em um arquivo  além de especificar também a quantidade de ocorrências desse termo. Essa posição inicial e quantidade são definidas nos atributos atributos `term_file_start_pos` e `doc_count_with_term`, respectivamente. A posição inicial está em bytes e, nesse exemplo, foi considerado que cada TermOcurrence possui 10 bytes. Assim, por exemplo, o termo casa (term_id=2) inicia-se na posição 20 e possui duas ocorrências. Com isso, é possível obter todas as ocorrências de um determinado termo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para deixarmos a estrutura dessa forma, temos um dificultador: no arquivo, temos que ordenar as ocorrências pelo termo, porém, indexamos por documento (veja na atividade 3). Assim, se gravássemos as ocorrências assim que indexarmos, as gravaríamos agrupadas por documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, temos que garantir uma ordenação por termo do arquivo externo, lembrando que nem sempre é possível armazenar todo o arquivo em memória principal. Para resolvermos isso, faremos o seguinte:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sempre, ao indexar, salvaremos o índice em uma lista (temporária) de ocorrência de termos `lst_occurrences_tmp`\n",
    "- Usaremos o método `save_tmp_occurrences` para, assim que a lista estiver com um determinado tamanho, ordená-la pelo termo e salvar de forma ordenada em um novo arquivo de indice com a todas as ocorrências. Para que seja feito isso, você deverá fazer uma ordenação externa levando em consideração o índice em arquivo atual e a lista de ocorrências temporárias. Segue o passo a passo:\n",
    "    - (1) ordene a lista `lst_occurrences_tmp`. Lembre-se que você implementou os comparadores das instâncias TermOccurrence, assim, a ordenação e descobrir o menor valor entre as ocorrências é uma operação simples;\n",
    "    - (2) criar um arquivo novo;\n",
    "    - (3) compare a primeira posição da lista com a primeira posição do arquivo de índice, sempre inserindo a ocorrência considerada com o menor entre elas no novo arquivo. Lembrando novamente que os comparadores foram implementados e que você possui os métodos `next_from_list` e `next_from_file` - que será implementado na atividade 5(b) para ajudar;\n",
    "    - (4) esse novo arquivo passará a ser o índice. Exclua o indice antigo e limpe a lista de ocorrencias `lst_occurrences_tmp`.\n",
    "- O método `finish_indexing` é o método que será chamado ao finalizar a indexação. Neste contexto ele será usado para organizar o `dic_index` atualizando os atributos `term_file_start_pos` e `doc_count_with_term` para os valores corretos.\n",
    "\n",
    "Na primeira execução, não haverá arquivo e você adicionará a lista toda no arquivo de forma sequencial. Fazendo esse procedimento, você sempre irá garantir um arquivo ordenado da forma esperada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5(a) - método de escrita no arquivo:** Iremos necessitar, em algum momento, a escrita de uma instância de `TermOccurrence` em arquivo. Assim deveremos implementar o método de escrita em arquivo nessa classe. Para economizar espaço e por simplicidade, será escrito em um arquivo binário armazenando os três atributos inteiros. Cada inteiro será armazenado em 4 bytes - será o suficiente para a nossa indexação. Veja um exemplo abaixo de escrita, leitura e impressão do posicionamento no arquivo (esses métodos serão úteis nessa e nas próximas atividades)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n4\nnúmero: 100\n"
     ]
    }
   ],
   "source": [
    "x = 100\n",
    "with open(\"xuxu.idx\",\"wb\") as file:\n",
    "    print(file.tell())\n",
    "    file.write(x.to_bytes(4,byteorder=\"big\"))\n",
    "    print(file.tell())\n",
    "with open(\"xuxu.idx\",\"rb\") as file:\n",
    "    print(f\"número: {int.from_bytes(file.read(4),byteorder='big')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5(b) - métodos next_from_file e next_from_list**: Antes de fazer a ordenação, é interessante implementar esses dois métodos que irão auxiliar na obtenção do menor elemento no arquivo e na lista, respectivamente. Considerando que a lista e o arquivo estão ordenados de forma crescente, temos que obter o primeiro elemento da lista e retirá-lo (utilize o [método pop](https://docs.python.org/3.1/tutorial/datastructures.html)).  \n",
    "\n",
    "Para a leitura do arquivo, iremos usar a API [pickle](https://docs.python.org/3/library/pickle.html) que facilita inserir/carregar estruturas em arquivo binário. O método `next_from_file` já possui um arquivo aberto e você deverá ler a próxima entrada por meio da função `load` da API `pickle`. Caso não exista próximo elemento, é lançado uma exceção. Em ambos os métodos, caso não exista próximo elemento, será retornado `None`. \n",
    "\n",
    "Complete a implementação substituindo os `None` quando julgar necessário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 7 - método save_tmp_occurrences: ** Implemente o método `save_tmp_occurrences`. Esse método deverá salvar a lista `lst_occurrences_tmp` em arquivo de forma ordenada, conforme explicado anteriormente. Neste método, você não precisa preocupar com o atributo `dic_index`. Leve em consideração os seguintes atributos/métodos:\n",
    "\n",
    "- `idx_file_counter`:  No código, você irá criar sempre novos índices, excluindo o antigo. Este atributo será útil para definirmos o nome do arquivo do índice. O novo arquivo do índice chamará `occur_index_X` em que $X$ é o número do mesmo. \n",
    "- `str_idx_file_name`: Atributo que armazena o arquivo índice atual. A primeira vez que executarmos `save_tmp_occurrences` não haverá arquivo criado e, assim `str_idx_file_name = None`\n",
    "- `lst_occurrences_tmp`: Lista de ocorrências a serem armazenadas em arquivo\n",
    "- `next_from_file` e `next_from_list`: Implementados na atividade anterior, para obter o próximo item do arquivo ou da lista. Importantes para fazer a ordenação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o teste unitário abaixo para verificar corretude deste código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Primeira execução (criação inicial do indice) [ok]\nInserção de alguns itens - teste 1/2 [ok]\nInserção de alguns itens - teste 2/2 [ok]\n.\n----------------------------------------------------------------------\nRan 1 test in 0.005s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "!python -m index.file_index_test FileIndexTest.test_save_tmp_occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 8: método finish_indexing: ** Agora, com as ocorrências organizadas no arquivo por termo, você deverá implementar o método `finish_indexing` para atualizar o atributo `dic_index` com a posição inicial e quantidade de documentos de cada termo nas suas instâncias `TermFilePosition`. Logo após, execute o teste unitário abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lista de ocorrências a serem testadas:\n(term_id:1 doc: 1 freq: 3)\n(term_id:1 doc: 2 freq: 2)\n(term_id:1 doc: 3 freq: 1)\n(term_id:2 doc: 1 freq: 1)\n(term_id:2 doc: 2 freq: 1)\n(term_id:2 doc: 3 freq: 2)\n(term_id:3 doc: 1 freq: 3)\n(term_id:4 doc: 1 freq: 5)\n(term_id:4 doc: 2 freq: 5)\nTamanho de cada ocorrência: 94 bytes\n.\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "!python -m index.file_index_test FileIndexTest.test_finish_indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 9 - implementação em `FileIndex` dos métodos abstratos da classe Index: ** Como vocês perceberam, `FileIndex` é subclasse de `Index`. Assim,  precisamos implementar os métodos abstratos da classe `Index`:\n",
    "\n",
    "- `create_index_entry`: no `FileIndex` para criar uma nova entrada no índice, você deverá retornar uma instância de `TermFilePosition` para este novo `term_id`. Ao criá-lo, você não precisa de definir a posição inicial do arquivo nem a quantidade de documentos. Conforme vocês implementaram nas atividades anteriores, isso é feito apenas no momento de finalização da indexação;\n",
    "- `add_index_occur`: Neste caso, você deverá criar e adicionar uma nova ocorrencia na lista de ocorrências temporárias `lst_occurrences_tmp` e, caso senha passado o limite `FileIndex.TMP_OCCURRENCES_LIMIT` do número máximo de ocorrências na lista, chamar o método `save_tmp_occurrence`.\n",
    "- `get_occurrence_list` e `document_count_with_term`: Possuem as mesmas funcionalidades descritas na atividade 3, porém, agora você deverá considerar a estrutura criada no `FileIndex`. Lembrem-se que esse método é só chamado após a finalização da indexação, assim, considere que o índice já está pronto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 10 - Teste unitário** Dessa vez, você deverá alterar uma classe de teste unitário para conseguir executá-la. \n",
    "\n",
    "Agora iremos testar os métodos get_occurrence_list e document_count_with_term. Lembre-se que já temos um teste unitário para isso, porém ele testa a estrutura de um `HashIndex`. Neste caso, iremos apenas mudar a estrutura, mas os métodos serão o mesmo, por isso, conseguiremos reaproveitar o teste feito anteriormente criando apenas uma nova classe de teste. \n",
    "\n",
    "Implementando este teste, você perceberá como é lindo usar orientação objetos ao seu favor 🥰. No arquivo `index_structure_test.py` você possui a classe `FileStructureTest` que é subclasse de nosso teste criado `StructureTest`.  Você deverá implementar o método `setUp` na classe `FileStructureTest` que sobrepõe o método de mesmo nome na classe `StructureTest`. O método `setUp` é executado sempre antes do teste. Este método que você irá criar irá fazer exatamente a mesma coisa que o criado em `StructureTest` porém, você deverá instanciar um `FileIndex` ao invés de um `HashIndex`. Logo após, execute os testes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======= Indice Gerado ======\ncasa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\nvermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\nverde -> [(term_id:2 doc: 1 freq: 1)]\n.\n----------------------------------------------------------------------\nRan 1 test in 0.002s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "!python -m index.index_structure_test  FileStructureTest.test_document_count_with_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======= Indice Gerado ======\ncasa -> [(term_id:0 doc: 1 freq: 10), (term_id:0 doc: 2 freq: 3)]\nvermelho -> [(term_id:1 doc: 1 freq: 3), (term_id:1 doc: 2 freq: 1), (term_id:1 doc: 3 freq: 1)]\nverde -> [(term_id:2 doc: 1 freq: 1)]\n.\n----------------------------------------------------------------------\nRan 1 test in 0.002s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "!python -m index.index_structure_test  FileStructureTest.test_get_occurrence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma similar, também criamos um teste de performance. Verifique a performance da indexação em arquivo e em memória ao indexar milhões de ocorrências de termos utilizando os testes abaixo. Note que, desta vez, estamos chamando os testes por meio de comandos Python e não pelo terminal. Assim, caso queira fazer alguma alteração no arquivos `.py`, você deve reiniciar o kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Índice completamente em memória principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memoria usada: 203.668598 MB; Máximo 203.66871 MB\n",
      "Indexando ocorrencia #1,250,000/1,250,000 (100%)\n",
      "Tempo gasto: 14.938929s\n",
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 15.884s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "import unittest\n",
    "from index.performance_test import PerformanceTest\n",
    "\n",
    "PerformanceTest.NUM_DOCS = 2500\n",
    "PerformanceTest.NUM_TERM_PER_DOC = 500\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(PerformanceTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Índice com ocorrências em memória secundária. Veja que, abaixo, que você pode ajustar o parâmetro de número de ocorrências em memória. Será muito útil para não gastar tanto tempo ao indexar o conteúdo da Wikipédia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memoria usada: 4.284433 MB; Máximo 5.929993 MB\n",
      "Indexando ocorrencia #1,000,000/1,250,000 (80%)\n",
      "Tempo gasto: 2519.054258s\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7876cf1857f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msuite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munittest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTestLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadTestsFromTestCase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFilePerformanceTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0munittest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextTestRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.8/unittest/runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mstartTestRun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mstopTestRun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stopTestRun'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/unittest/suite.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/unittest/suite.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, result, debug)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/unittest/case.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/unittest/case.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    674\u001b[0m                 \u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpecting_failure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpecting_failure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestPartExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misTest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callTestMethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestMethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m                 \u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpecting_failure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestPartExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/unittest/case.py\u001b[0m in \u001b[0;36m_callTestMethod\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_callTestMethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_callTearDown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/SearchEnginesForWikipedia/index/performance_test.py\u001b[0m in \u001b[0;36mtest_performance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtracemalloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtracemalloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/SearchEnginesForWikipedia/index/performance_test.py\u001b[0m in \u001b[0;36mindex_words\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0midx_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mstr_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_term\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0;31m#indiceTeste.index(vocabulario[(count+1)%15625], d, (count%10)+1);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/SearchEnginesForWikipedia/index/structure.py\u001b[0m in \u001b[0;36mindex\u001b[0;34m(self, term, doc_id, term_freq)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         self.add_index_occur(\n\u001b[0;32m---> 23\u001b[0;31m             self.dic_index[term], doc_id, int_term_id, term_freq)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/SearchEnginesForWikipedia/index/structure.py\u001b[0m in \u001b[0;36madd_index_occur\u001b[0;34m(self, entry_dic_index, doc_id, term_id, term_freq)\u001b[0m\n\u001b[1;32m    164\u001b[0m             TermOccurrence(doc_id, term_id, term_freq))\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlst_occurrences_tmp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mFileIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTMP_OCCURRENCES_LIMIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tmp_occurrences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/SearchEnginesForWikipedia/index/structure.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, doc_id, term_id, term_freq)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from index.performance_test import FilePerformanceTest,PerformanceTest\n",
    "from index.structure import FileIndex\n",
    "\n",
    "PerformanceTest.NUM_DOCS = 2500\n",
    "PerformanceTest.NUM_TERM_PER_DOC = 500\n",
    "FileIndex.TMP_OCCURRENCES_LIMIT = 10000\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(FilePerformanceTest)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexador de HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, você irá alterar o arquivo `indexer.py` para [preprocessar conteúdo HTML](https://docs.google.com/presentation/d/1C22jQWIYobiqMx8SmP1y2lr1uSlvJSu3ayu5lXC5d8A/edit?usp=sharing) e depois indexá-lo. Com isso, você poderá usá-lo para indexação das páginas HTML, como os da Wikipédia. A classe `Cleaner` será responsável pelo pré-processamento e a HTMLIndexer, para a indexação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fadoaglauss/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/fadoaglauss/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from index.indexer import *\n",
    "#importamos o módulo structure\n",
    "#novamente para não precisar de executar o código do início da tarefa\n",
    "from index.structure import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 11 - Limpeza dos dados com a classe Cleaner: ** A classe `Cleaner` é responsável por prá-processar o conteúdo HTML para que ele esteja preparado para indexação. Essa classe tem alguns _flags_ para definir se algum tipo de processamento opcional será feito (por exemplo, _stemming_ e remoção de _stopwords_). Para isso, você deverá implementar pequenos métodos para fazer a limpeza. Esses códigos são pequenos pois temos lindas APIs para nos ajudar 💕. Você irá fazer o processamento básico e, se quiser, pode melhorar a implementação criando exceções na remoção de acentos e não retirando maiúsculas e minúsculas de certas palavras e unindo palavras compostas, por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada tarefa, há um método para ser criado a seguir os testes iniciais serão feitos aqui no Jupyter. Não esqueça de reiniciar o kernel sempre que alterar algo no código. Logo após, haverá um [teste de integração](https://en.wikipedia.org/wiki/Integration_testing) para avaliar a indexação como um todo.\n",
    "\n",
    "- **Transformação de HTML para texto: ** Na limpeza dos dados, iremos remover tudo que não será indexado - ou seja, o código HTML. Para isso, iremos implementar o método `html_to_plain_text` que transformará o HTML em texto corrido. Você pode utilizar o [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) para isso e o método get_text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'© oi! Meu nome é Hasan'"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "cleaner_test = Cleaner(stop_words_file=\"stopwords.txt\",\n",
    "                        language=\"portuguese\",\n",
    "                        perform_stop_words_removal=True,\n",
    "                        perform_accents_removal=True,\n",
    "                        perform_stemming=True)\n",
    "cleaner_test.html_to_plain_text(\"&copy; oi! Meu nome é <strong>Hasan</strong>\")\n",
    "#esperado: '© oi! Meu nome é Hasan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Verifica se é stopword**: O método `is_stopword` retorna verdadeiro se uma palavra, passada como parâmetro, é stopword. Para isso, você irá usar o atributo `set_stop_words`. Este atributo foi inicializado com um conjunto de stopwords de um arquivo. Esse arquivo, para testes, tem poucas stopwords. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False, esperado: False\nFalse, esperado: False\nTrue, esperado: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"{cleaner_test.is_stop_word('japão')}, esperado: False\")\n",
    "print(f\"{cleaner_test.is_stop_word('cama')}, esperado: False\")\n",
    "print(f\"{cleaner_test.is_stop_word('é')}, esperado: True\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stemming**: você deverá implementar o método word_stem para realizar o stemming. Você deverá usar a [classe SnowballStemmer da API NLTK](https://www.nltk.org/howto/stem.html). Um objeto dessa classe já está instanciado no atributo `stemmer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "verdad, esperado: verdad\nestud, esperado: estud\namad, esperado: amad\n"
     ]
    }
   ],
   "source": [
    "print(f\"{cleaner_test.word_stem('verdade')}, esperado: verdad\")\n",
    "print(f\"{cleaner_test.word_stem('estudante')}, esperado: estud\")\n",
    "print(f\"{cleaner_test.word_stem('amado')}, esperado: amad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remoção de acentos:** Iremos fazer de uma forma bem simples a remoção de acentos: aplicando uma tabela de substituição de caracteres. Para isso, você deverá criar uma [tabela de tradução](https://docs.python.org/3.3/library/stdtypes.html?highlight=maketrans#str.maketrans) no atributo `accents_translation_table` baseando-se nas variáveis `in_table` e `out_table` também presentes no construtor (substitua o None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cancao, esperado: cancao\neletrico, esperado: eletrico\namado, esperado: amado\n"
     ]
    }
   ],
   "source": [
    "print(f\"{cleaner_test.remove_accents('canção')}, esperado: cancao\")\n",
    "print(f\"{cleaner_test.remove_accents('elétrico')}, esperado: eletrico\")\n",
    "print(f\"{cleaner_test.remove_accents('amado')}, esperado: amado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora você irá fazer o método `preprocess_word` ele irá receber como parâmetro uma palavra e irá verificar se é uma palavra válida de ser indexada. Caso não seja, retornará None. Caso contrário, irá retornar a palavra pré-rocessada. Uma palavra válida a ser indexada é aquela que não é pontuação e não é stopword (caso `perform_stop_words_removal = True`). Para que seja feito o pré-processamento você deverá: transformar o texto para minúsculas, remover acento (se `perform_accents_removal=True`), fazer o stemming (se `perform_stemming = True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 12 - Classe HTMLIndexer - método text_word_count: **  Você deverá implementar o método `text_word_count`, a partir de um texto. Esse método retorna um dicionário em que, para cada palavra no texto, será apresentado sua frequência. Considere que o texto já está limpo e é necessário fazer apenas o processamento das palavras.\n",
    "\n",
    "Para isso, você deverá:  dividir o texto em tokens (que, no nosso caso, são as palavras e pontuações); pré-processar cada palavra usando o `HTMLIndexer.cleaner`; e, se for uma palavra válida, contabilizá-la. Para isso, será necessário [o método word_tokenize da API NLTK](https://kite.com/python/docs/nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ola': 1, 'qual': 1, 'e': 1, 'o': 1, 'dad': 2, 'que': 1, 'precis': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "index = HashIndex()\n",
    "indexador_teste = HTMLIndexer(index)\n",
    "indexador_teste.text_word_count(\"Olá! Qual é o dado dado que precisa?\")\n",
    "#esperado:\n",
    "#{'dad': 2, 'o': 1, 'ola': 1, 'precis': 1, 'qual': 1, 'que': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 13 - método index_text: ** Implemente o método `index_text` que deverá (1) converter o HTML para texto simples usando `HTMLIndexer.cleaner`; (2) converter o texto em um dicionário de ocorrências de palavras com sua frequência (metodo da atividade 12); e (3) indexar cada palavra deste dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ola': [(term_id:0 doc: 10 freq: 1)],\n",
       " 'qua': [(term_id:1 doc: 10 freq: 1)],\n",
       " 'sao': [(term_id:2 doc: 10 freq: 1)],\n",
       " 'os': [(term_id:3 doc: 10 freq: 1)],\n",
       " 'dad': [(term_id:4 doc: 10 freq: 1)],\n",
       " 'que': [(term_id:5 doc: 10 freq: 1)],\n",
       " 'precis': [(term_id:6 doc: 10 freq: 1)]}"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "\n",
    "index = HashIndex()\n",
    "indexador_teste = HTMLIndexer(index)\n",
    "#o HTML está mal formado de propósito ;)\n",
    "indexador_teste.index_text(10,\"<strong>Ol&aacute;! </str> Quais são os dados que precisará?\")\n",
    "\n",
    "indexador_teste.index.dic_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esperado:\n",
    "<pre>\n",
    "{'dad': [(term_id:4 doc: 10 freq: 1)],\n",
    " 'ola': [(term_id:0 doc: 10 freq: 1)],\n",
    " 'os': [(term_id:3 doc: 10 freq: 1)],\n",
    " 'precis': [(term_id:6 doc: 10 freq: 1)],\n",
    "'qua': [(term_id:1 doc: 10 freq: 1)],\n",
    " 'que': [(term_id:5 doc: 10 freq: 1)],\n",
    " 'sao': [(term_id:2 doc: 10 freq: 1)]}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 14: Indexação de um diretorio com subdiretorios** Você deverá implementar o método `index_text_dir` que, dado um diretório, navega em todos os seus subdiretórios e indexa todos os arquivos HTMLs. Considere que os arquivos sejam sempre nomeados pelo seu ID. Veja o exemplo em `doc_test`. Logo após, execute o teste unitário para ver a corretude do seu indexador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fadoaglauss/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/fadoaglauss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "F\n",
      "======================================================================\n",
      "FAIL: test_indexer (__main__.IndexerTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fadoaglauss/Projects/SearchEnginesForWikipedia/index/indexer_test.py\", line 18, in test_indexer\n",
      "    self.assertTrue(len(sobra_expected) == 0 and len(sobra_vocab) == 0,\n",
      "AssertionError: False is not true : O Vocabulário indexado não é o esperado!\n",
      "Vocabulario indexado: {'verd', 'nao', 'cas', 'e', 'eis', 'ou', 'a', 'questa', 'ser'}\n",
      "Vocabulário esperado: {'verd', 'nao', 'cas', 'eis', 'ou', 'a', 'questa', 'ser'}\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.048s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "!python -m index.indexer_test IndexerTest.test_indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, para fazer a especificação do projeto, você deve baixar o [dataset da Wikipédia](https://drive.google.com/file/d/18zjhUvL07rT26f6nqziOK_MRS_4fnxLm/view?usp=sharing) e indexá-lo. Você deve também achar um arquivo de stopwords com mais termos aos que feito neste teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wikiSample'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ff70d068e04a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mobj_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhtml_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHTMLIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhtml_indexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_text_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wikiSample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracemalloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_traced_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtime_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/SearchEnginesForWikipedia/index/indexer.py\u001b[0m in \u001b[0;36mindex_text_dir\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindex_text_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstr_sub_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mpath_sub_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{path}/{str_sub_dir}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstr_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_sub_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wikiSample'"
     ]
    }
   ],
   "source": [
    "from index.indexer import *\n",
    "from index.structure import *\n",
    "import tracemalloc\n",
    "time_first = datetime.now()\n",
    "obj_index = FileIndex()\n",
    "html_indexer = HTMLIndexer(obj_index)\n",
    "html_indexer.index_text_dir(\"wikiSample\")  \n",
    "current, peak = tracemalloc.get_traced_memory()            \n",
    "time_end = datetime.now()\n",
    "tempo_gasto = time_end-time_first \n",
    "print(f\"Memoria usada: {current / 10**6:,} MB; Máximo {peak / 10**6:,} MB\")\n",
    "print(f\"Finalizado:{tempo_gasto.total_seconds()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "soma = 0\n",
    "with open(\"tempos.txt\",\"r\",encoding=\"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        a+=1\n",
    "        soma+=  float(line.split(\":\")[1])\n",
    "\n",
    "    print(f\"Qtd arquivos: {a} Soma total: {soma} media:{(soma/a)} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Recuperando o indice\n",
    "from index.indexer import *\n",
    "from index.structure import *\n",
    "import pickle\n",
    "with open(\"occur_index_14.idx\",\"rb\") as idx_file:\n",
    "    obj_index = FileIndex()\n",
    "    t = obj_index.next_from_file(idx_file)\n",
    "    while t != None:\n",
    "        print(t)\n",
    "        t = obj_index.next_from_file(idx_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}